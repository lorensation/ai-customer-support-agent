# AI Customer Support Agent - Frontend

> **Next.js 14 + LangChain + Vercel AI SDK** â€¢ Production-ready AI support with RAG & Web Search

A sophisticated customer support chatbot built with Next.js 14 (App Router), featuring intelligent tool orchestration via LangChain, streaming responses with Vercel AI SDK, and structured validation with Zod.

![Architecture: RAG with Web Search Fallback](https://img.shields.io/badge/Architecture-RAG%20%2B%20Web%20Search-blue)
![Stack: Next.js 14 | LangChain | OpenAI](https://img.shields.io/badge/Stack-Next.js%2014%20%7C%20LangChain%20%7C%20OpenAI-green)

---

## ğŸ¯ Features

### ğŸ¤– **AI-Powered Support**
- **Intelligent RAG Pipeline**: Semantic search through Supabase vector store
- **Web Search Fallback**: Automatic fallback to Tavily when confidence is low
- **Tool Orchestration**: LangChain agents dynamically select optimal tools
- **Streaming Responses**: Real-time response generation with Vercel AI SDK

### ğŸ”’ **Production-Ready**
- **Type-Safe**: Full TypeScript with Zod schema validation
- **Error Handling**: Comprehensive error boundaries and retry logic
- **Responsive UI**: Mobile-first design with Tailwind CSS
- **Accessible**: Built with Radix UI primitives

### ğŸ¨ **Modern UI**
- **Clean Chat Interface**: Inspired by modern chat applications
- **Persistent Sidebar**: Product info and navigation always accessible
- **Source Citations**: Transparent source attribution for all responses
- **Dark Mode Ready**: Full theme support (light/dark)

---

## ğŸ“ Project Structure

```
frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ ask/
â”‚   â”‚   â”‚       â””â”€â”€ route.ts           # Main API endpoint with LangChain agent
â”‚   â”‚   â”œâ”€â”€ globals.css                # Global styles & theme
â”‚   â”‚   â”œâ”€â”€ layout.tsx                 # Root layout
â”‚   â”‚   â””â”€â”€ page.tsx                   # Home page
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ui/                        # shadcn/ui components
â”‚   â”‚   â”‚   â”œâ”€â”€ button.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ input.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ card.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ avatar.tsx
â”‚   â”‚   â”‚   â””â”€â”€ scroll-area.tsx
â”‚   â”‚   â”œâ”€â”€ Chat.tsx                   # Main chat component (useChat hook)
â”‚   â”‚   â”œâ”€â”€ MessageBubble.tsx          # Individual message display
â”‚   â”‚   â””â”€â”€ Sidebar.tsx                # Product info sidebar
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ langchainTools.ts          # Retrieval & web search tools
â”‚       â”œâ”€â”€ zodSchemas.ts              # Validation schemas & types
â”‚       â””â”€â”€ utils.ts                   # Utility functions
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ tailwind.config.ts
â”œâ”€â”€ next.config.mjs
â”œâ”€â”€ postcss.config.mjs
â””â”€â”€ .env.local.example
```

---

## ğŸ§  Architecture Overview

### Tool-Calling Decision Flow

```
User Query
    â†“
[Intent Analysis]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retrieval Tool  â”‚ â† Always tried first
â”‚ (Supabase RAG)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
   [Confidence Check]
         â†“
    < 0.6 or No Results?
         â†“
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚   YES   â”‚â”€â”€â†’  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ Web Search Tool  â”‚
                    â”‚  (Tavily API)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                    [Combine Results]
                             â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  LLM Synthesis â”‚
                    â”‚  (GPT-4 Turbo) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                    [Stream to Client]
```

### How Tool Selection Works

The system uses a **smart fallback strategy**:

1. **Primary: Retrieval Tool**
   - Queries the existing Node.js backend (`/api/ask`)
   - Backend uses Supabase pgvector for semantic search
   - Intent-based filtering for precise results
   - Returns documents with similarity scores

2. **Secondary: Web Search Tool**
   - **Triggered when**:
     - Retrieval confidence < 0.6
     - No documents found in knowledge base
     - Query contains keywords like "latest", "recent", "current"
   - Uses Tavily API for real-time web search
   - Provides external, up-to-date information

3. **Synthesis**
   - LLM combines both sources intelligently
   - Prioritizes knowledge base for official info
   - Augments with web search for context
   - Cites all sources for transparency

---

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** 18+ 
- **Backend Running**: Your existing Node.js backend at `http://localhost:3000`
- **API Keys**:
  - OpenAI API key
  - Tavily API key (free at [tavily.com](https://tavily.com))
  - Supabase credentials (from backend)

### Installation

1. **Navigate to frontend directory**
   ```bash
   cd frontend
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Configure environment**
   ```bash
   cp .env.local.example .env.local
   ```

   Edit `.env.local`:
   ```env
   # Backend API (your Node.js Express server)
   NEXT_PUBLIC_BACKEND_URL=http://localhost:3000

   # OpenAI API Key (for LangChain tools)
   OPENAI_API_KEY=sk-your-actual-key

   # Supabase Configuration (same as backend)
   SUPABASE_URL=https://your-project.supabase.co
   SUPABASE_ANON_KEY=your-supabase-anon-key

   # Web Search API (Tavily)
   TAVILY_API_KEY=tvly-your-actual-key

   # Optional: Confidence threshold (default 0.6)
   CONFIDENCE_THRESHOLD=0.6
   ```

4. **Start development server**
   ```bash
   npm run dev
   ```

5. **Open browser**
   ```
   http://localhost:3001
   ```
   *(Uses 3001 to avoid conflict with backend on 3000)*

---

## ğŸ”§ Configuration

### Backend Integration

The frontend connects to your existing Node.js backend:

**Backend Endpoint Used**: `POST /api/ask`

**Request Format**:
```json
{
  "query": "How do I reset my password?"
}
```

**Response Format** (from your backend):
```json
{
  "success": true,
  "answer": "To reset your password...",
  "sources": [
    {
      "id": 1,
      "filename": "faq.md",
      "similarity": "0.856",
      "excerpt": "Password reset instructions..."
    }
  ],
  "metadata": {
    "documentsRetrieved": 3,
    "model": "gpt-4-turbo-preview",
    "timestamp": "2025-11-02T..."
  }
}
```

### Tool Configuration

**Retrieval Tool** (`langchainTools.ts`):
```typescript
// Queries your backend's /api/ask endpoint
createRetrievalTool(BACKEND_URL)
  - Uses intent classification
  - Returns top-K similar documents
  - Includes similarity scores
```

**Web Search Tool** (`langchainTools.ts`):
```typescript
// Tavily API integration
createWebSearchTool(TAVILY_API_KEY)
  - max_results: 3
  - search_depth: "basic"
  - include_answer: true
```

### Confidence Threshold

Control when web search is triggered:

```typescript
// In .env.local
CONFIDENCE_THRESHOLD=0.6  // Default

// Higher = More conservative (less web search)
// Lower = More aggressive (more web search)
```

---

## ğŸ¨ Customization

### Branding

**Update Sidebar** (`components/Sidebar.tsx`):
```typescript
<h1 className="text-xl font-bold">Your Brand Name</h1>
<p className="text-xs">Your Tagline</p>
```

**Colors** (`tailwind.config.ts`):
```typescript
extend: {
  colors: {
    primary: "hsl(221.2 83.2% 53.3%)",  // Change primary color
    // ... other theme colors
  }
}
```

### System Prompt

**Customize Agent Behavior** (`app/api/ask/route.ts`):
```typescript
const AGENT_SYSTEM_PROMPT = `
You are [Your Brand]'s AI support agent.
Your mission: ...
Your tone: ...
Your constraints: ...
`;
```

### Welcome Message

**Update Example Questions** (`components/Chat.tsx`):
```typescript
const exampleQuestions = [
  "Your question 1",
  "Your question 2",
  // ...
];
```

---

## ğŸš¢ Deployment

### Deploy to Vercel

1. **Push to GitHub**
   ```bash
   git add .
   git commit -m "Add frontend"
   git push
   ```

2. **Connect to Vercel**
   - Go to [vercel.com](https://vercel.com)
   - Import your repository
   - Root Directory: `frontend`

3. **Configure Environment Variables**
   Add all variables from `.env.local` in Vercel dashboard

4. **Deploy**
   - Vercel auto-deploys on push
   - Production URL: `https://your-app.vercel.app`

### Backend Hosting

Your Node.js backend needs to be accessible:

**Options**:
- **Railway**: Easy Node.js hosting
- **Render**: Free tier available
- **Fly.io**: Edge deployment
- **AWS/GCP**: Full control

**Update Frontend**:
```env
NEXT_PUBLIC_BACKEND_URL=https://your-backend.railway.app
```

---

## ğŸ“Š Monitoring & Debugging

### LangChain Tracing

Enable detailed tool execution logs:

```env
# .env.local
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your-langsmith-key
```

Visit [smith.langchain.com](https://smith.langchain.com) to view:
- Tool invocations
- Execution times
- Input/output logs
- Error traces

### Console Logs

The app includes comprehensive logging:

```typescript
// Retrieval Tool
console.log(`ğŸ” [Retrieval Tool] Searching for: "${query}"`);
console.log(`âœ… [Retrieval Tool] Retrieved ${docs.length} documents`);

// Web Search Tool
console.log(`ğŸŒ [Web Search Tool] Searching web for: "${query}"`);
console.log(`âœ… [Web Search Tool] Found ${results.length} results`);
```

---

## ğŸ§ª Testing

### Test Retrieval Flow

```bash
# Terminal 1: Backend
cd ..
npm start

# Terminal 2: Frontend
npm run dev
```

**Test Query**: "How do I get started?"
- Should use retrieval tool only
- Check console for tool execution
- Sources should cite knowledge base

### Test Web Search Fallback

**Test Query**: "What are the latest AI trends?"
- Should trigger web search (no KB results)
- Check console for fallback logic
- Sources should include web URLs

### Test Hybrid Approach

**Test Query**: "Compare our security with industry standards"
- Should use both tools
- Retrieval for internal docs
- Web search for industry context

---

## ğŸ“š API Reference

### POST `/api/ask`

**Request**:
```typescript
{
  query: string;        // User question (3-1000 chars)
  sessionId?: string;   // Optional session tracking
  metadata?: object;    // Optional metadata
}
```

**Response** (Streaming):
```
data: AI is analyzing your question...
data: Based on our documentation...
data: [DONE]
```

**Error Response**:
```json
{
  "success": false,
  "error": "Internal server error",
  "message": "Detailed error message"
}
```

---

## ğŸ› ï¸ Tech Stack Details

| Category | Technology | Purpose |
|----------|-----------|---------|
| **Framework** | Next.js 14 (App Router) | React framework with server components |
| **AI SDK** | Vercel AI SDK | Streaming, chat state management |
| **AI Orchestration** | LangChain | Tool calling, agent workflows |
| **LLM** | OpenAI GPT-4 Turbo | Response generation |
| **Embeddings** | text-embedding-3-small | Vector search (backend) |
| **Vector DB** | Supabase pgvector | Document storage (backend) |
| **Web Search** | Tavily API | Real-time search fallback |
| **Validation** | Zod | Type-safe schemas |
| **UI Components** | Radix UI + shadcn/ui | Accessible components |
| **Styling** | Tailwind CSS | Utility-first CSS |
| **Icons** | Lucide React | Icon library |
| **Type Safety** | TypeScript | Full type coverage |

---

## ğŸ“– How It Works: Tool Calling Narrative

When a user sends a question, here's the intelligent decision-making process:

### 1. **Initial Query Reception**
The `useChat()` hook sends the query to `/api/ask`, which initializes a LangChain agent with two tools: `retrieval_tool` and `web_search_tool`.

### 2. **Primary: Knowledge Base Search**
The agent **always starts** with the retrieval tool, which calls your existing Node.js backend. The backend:
- Generates embeddings for the query
- Performs vector similarity search in Supabase
- Applies intent-based filtering (API docs, billing, FAQ, etc.)
- Returns top-K documents with confidence scores

### 3. **Decision Point: Confidence Evaluation**
The agent evaluates the retrieval results:

**HIGH CONFIDENCE (â‰¥0.6)** â†’ Response generated from knowledge base only
```
âœ… Retrieved 5 documents (avg: 0.82 confidence)
âœ… Sufficient information found
âœ… Response generated from internal sources
```

**LOW CONFIDENCE (<0.6)** â†’ Triggers web search fallback
```
âš ï¸ Retrieved 1 document (avg: 0.45 confidence)
âš ï¸ Triggering web search for additional context...
ğŸŒ Searching Tavily API...
âœ… Combined knowledge base + web results
```

### 4. **Synthesis & Streaming**
The LLM:
- Combines information from all sources
- Prioritizes official knowledge base data
- Augments with web search for completeness
- Streams response token-by-token to the client
- Cites all sources used

### 5. **Client Display**
The frontend:
- Displays streaming response in real-time
- Shows source chips for transparency
- Handles errors gracefully with retry
- Maintains conversation context

---

## ğŸ¤ Contributing

Found a bug or have a feature request? PRs welcome!

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](../LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **LangChain**: Powerful AI orchestration framework
- **Vercel AI SDK**: Seamless streaming & chat state
- **Tavily**: Excellent web search API
- **shadcn/ui**: Beautiful component library
- **Supabase**: Scalable vector database

---

## ğŸ“ Support

- **Email**: support@example.com
- **Issues**: [GitHub Issues](https://github.com/yourusername/your-repo/issues)
- **Docs**: See `/docs` folder in backend

---

**Built with â¤ï¸ using LangChain + Next.js + Vercel AI SDK**
